{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTTYJhundF7s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "-->In machine learning, a parameter is a configuration variable that is internal to the model and whose value is estimated from the training data.\n",
        "\n",
        "Examples include the weights and biases in a neural network or the coefficients in linear regression.\n",
        "\n",
        "2. What is correlation?\n",
        "-->Correlation measures the statistical relationship or association between two variables.\n",
        "\n",
        "It indicates how one variable changes with respect to another.\n",
        "\n",
        "It is often represented by the correlation coefficient (r), which ranges from -1 to 1.\n",
        "\n",
        "What does negative correlation mean?\n",
        "A negative correlation means that as one variable increases, the other decreases.\n",
        "\n",
        "A correlation coefficient near -1 indicates a strong negative correlation.\n",
        "\n",
        "Example: As exercise time increases, body fat percentage decreases.\n",
        "\n",
        "3. Define Machine Learning.\n",
        "-->Machine Learning is a branch of artificial intelligence where computers learn patterns from data and make decisions or predictions without being explicitly programmed.\n",
        "\n",
        "It involves training models on data to improve their performance on a specific task.\n",
        "\n",
        "What are the main components in Machine Learning?\n",
        "-->The main components are:\n",
        "\n",
        "Data – Input used to train the model.\n",
        "\n",
        "Model – The algorithm or function that makes predictions.\n",
        "\n",
        "Features – Individual measurable properties or characteristics used as input.\n",
        "\n",
        "Labels – The target values we want the model to predict.\n",
        "\n",
        "Training – The process of feeding data into the model so it can learn.\n",
        "\n",
        "Loss function – Measures how far off the predictions are from actual values.\n",
        "\n",
        "Optimization algorithm – Adjusts the model parameters to minimize loss (e.g., gradient descent).\n",
        "\n",
        "4.How does loss value help in determining whether the model is good or not?\n",
        "-->The loss value is a number that indicates how far the model’s predictions are from the actual values.\n",
        "\n",
        "Lower loss means better performance.\n",
        "\n",
        "It helps during training to know whether the model is improving.\n",
        "\n",
        "However, it should be paired with other metrics (accuracy, precision, recall) to fully assess performance.\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "\n",
        "-->Continuous variables are numeric and can take any value within a range.\n",
        "\n",
        "Examples: height, temperature, age.\n",
        "\n",
        "Categorical variables represent discrete categories or groups.\n",
        "\n",
        "Examples: gender (male/female), color (red/blue/green), type of car.\n",
        "\n",
        "6.How do we handle categorical variables in Machine Learning? What are the common t echniques?\n",
        "\n",
        "-->\n",
        "Handling categorical variables is a crucial step in preparing data for machine learning, as most algorithms require numerical input. Here are the common techniques to handle categorical variables:\n",
        "\n",
        "🔹 1. Label Encoding\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "Color: Red → 0, Blue → 1, Green → 2\n",
        "\n",
        "✅ Useful for ordinal categories (where order matters).\n",
        "\n",
        "⚠️ Can mislead algorithms into thinking there's an ordinal relationship if there isn't.\n",
        "\n",
        "🔹 2. One-Hot Encoding\n",
        "Creates a new binary column for each category.\n",
        "\n",
        "Example (Color: Red, Blue, Green):\n",
        "\n",
        "css\n",
        "Copy\n",
        "Edit\n",
        "Red   → [1, 0, 0]  \n",
        "Blue  → [0, 1, 0]  \n",
        "Green → [0, 0, 1]\n",
        "✅ Best for nominal categories (no natural order).\n",
        "\n",
        "⚠️ Can lead to high-dimensional data if there are many categories (curse of dimensionality).\n",
        "\n",
        "🔹 3. Ordinal Encoding\n",
        "Similar to label encoding but applied when the categories have a clear, ranked order.\n",
        "\n",
        "Example (Size: Small → 0, Medium → 1, Large → 2).\n",
        "\n",
        "✅ Preserves order information.\n",
        "\n",
        "⚠️ Don't use if categories are unordered — it may introduce bias.\n",
        "\n",
        "🔹 4. Target Encoding (Mean Encoding)\n",
        "Replaces each category with the mean of the target variable for that category.\n",
        "\n",
        "Example: If in a binary classification task, City A has 70% of positive class, encode City A as 0.7.\n",
        "\n",
        "✅ Can be powerful with lots of data.\n",
        "\n",
        "⚠️ Prone to data leakage if not done carefully (e.g., use cross-validation).\n",
        "\n",
        "🔹 5. Binary Encoding\n",
        "Combines the benefits of label and one-hot encoding.\n",
        "\n",
        "Converts category labels into binary code and splits the digits into separate columns.\n",
        "\n",
        "✅ More compact than one-hot, useful for high-cardinality features.\n",
        "\n",
        "🔹 6. Frequency or Count Encoding\n",
        "Replaces each category with the count or frequency of its occurrence.\n",
        "\n",
        "✅ Simple, and sometimes effective when frequency correlates with the target.\n",
        "\n",
        "⚠️ Can overemphasize frequent categories if not normalized.\n",
        "\n",
        "7.What do you mean by training and testing a dataset?\n",
        "-->In machine learning, the dataset is usually split into two main parts:\n",
        "\n",
        "Training Set\n",
        "\n",
        "Used to train the model (i.e., help it learn patterns from the data).\n",
        "\n",
        "The model adjusts its internal parameters based on this data.\n",
        "\n",
        "Testing Set\n",
        "\n",
        "Used to evaluate how well the trained model performs on new, unseen data.\n",
        "\n",
        "It helps check for overfitting (when the model learns too much detail from training data and fails to generalize).\n",
        "\n",
        "8.What is sklearn.preprocessing?\n",
        "-->>sklearn.preprocessing is a module from Scikit-learn, a popular Python library for machine learning. This module contains tools for preparing and transforming data, including:\n",
        "\n",
        "Scaling (e.g., StandardScaler, MinMaxScaler) – to normalize or standardize numeric values.\n",
        "\n",
        "Encoding (e.g., OneHotEncoder, LabelEncoder) – to convert categorical variables into numbers.\n",
        "\n",
        "Imputation (e.g., SimpleImputer) – to fill in missing values.\n",
        "\n",
        "PolynomialFeatures – to create new features from existing ones.\n",
        "\n",
        "9. What is a Test Set?\n",
        "\n",
        "-->A test set is a subset of your dataset that:\n",
        "\n",
        "Is not used during training.\n",
        "\n",
        "Is only used after training, to evaluate how well the model performs.\n",
        "\n",
        "Simulates how the model would perform on real-world data.\n",
        "\n",
        "10.How do we split data for model fitting (training and testing) in Python?\n",
        "-->from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppose you have features (X) and target labels (y)\n",
        "X = ...  # your input features (e.g., a DataFrame or NumPy array)\n",
        "y = ...  # your target variable (e.g., a column of labels)\n",
        "\n",
        "# Split the data: 80% for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "Parameters:\n",
        "test_size: Proportion of the data to use for testing (e.g., 0.2 = 20%).\n",
        "\n",
        "random_state: A seed for reproducibility (use the same value to get the same split every time).\n",
        "\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "1. Understand the Problem\n",
        "What is the objective (classification, regression, etc.)?\n",
        "\n",
        "What outcome are we trying to predict?\n",
        "\n",
        "What is the business context or goal?\n",
        "\n",
        "2. Collect and Explore the Data\n",
        "Gather the dataset.\n",
        "\n",
        "Use exploratory data analysis (EDA):\n",
        "\n",
        "pandas, matplotlib, seaborn\n",
        "\n",
        "Look at distributions, missing values, correlations, etc.\n",
        "\n",
        "3. Preprocess the Data\n",
        "Handle missing values (SimpleImputer)\n",
        "\n",
        "Encode categorical variables (OneHotEncoder, LabelEncoder)\n",
        "\n",
        "Normalize or scale numerical features (StandardScaler, MinMaxScaler)\n",
        "\n",
        "Split into training and test sets (train_test_split)\n",
        "\n",
        "4. Choose a Model\n",
        "Based on the problem type:\n",
        "\n",
        "Classification: LogisticRegression, RandomForestClassifier, etc.\n",
        "\n",
        "Regression: LinearRegression, GradientBoostingRegressor, etc.\n",
        "\n",
        "5. Train the Model\n",
        "model.fit(X_train, y_train)\n",
        " 6.6. Evaluate the Model\n",
        "Use the test set to measure performance:\n",
        "\n",
        "Classification: accuracy, precision, recall, F1-score\n",
        "\n",
        "Regression: MAE, MSE, RMSE, R²\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "7. Tune Hyperparameters\n",
        "Use techniques like Grid Search or Randomized Search to improve model performance.\n",
        "\n",
        "8. Deploy the Model (optional)\n",
        "Use the model in a real-world application (e.g., web app, API).\n",
        "\n",
        "Summary Checklist ✅:\n",
        " Define problem\n",
        "\n",
        " Load data\n",
        "\n",
        " Clean and preprocess\n",
        "\n",
        " Split data\n",
        "\n",
        " Train model\n",
        "\n",
        " Evaluate model\n",
        "\n",
        " Tune and improve\n",
        "\n",
        " (Optional) Deploy model\n",
        "\n",
        " 11.Why do we perform EDA before fitting a model to the data?\n",
        "\n",
        "-->Exploratory Data Analysis (EDA) helps you understand the structure, patterns, and relationships in your data before building a model.\n",
        "\n",
        "🔍 Main Goals of EDA:\n",
        "Detect data quality issues: Missing values, outliers, inconsistent entries.\n",
        "\n",
        "Understand feature distributions: Are they skewed, normal, uniform?\n",
        "\n",
        "Identify relationships: Between features and with the target.\n",
        "\n",
        "Select important variables: See which features matter most.\n",
        "\n",
        "Avoid pitfalls: Like data leakage, bias, or incorrect assumptions.\n",
        "\n",
        "➡️ Without EDA, you risk feeding poor data into your model, which often leads to misleading or poor results.\n",
        "\n",
        "12.What is correlation?\n",
        "Correlation is a statistical measure that shows the strength and direction of a relationship between two variables.\n",
        "\n",
        "Measured by the correlation coefficient (r):\n",
        "\n",
        "Range: -1 to 1\n",
        "\n",
        "+1: Perfect positive correlation\n",
        "\n",
        "0: No correlation\n",
        "\n",
        "-1: Perfect negative correlation\n",
        "\n",
        "13.What does negative correlation mean?\n",
        "A negative correlation means that as one variable increases, the other decreases.\n",
        "\n",
        "Example:\n",
        "As outside temperature increases, heating bills tend to decrease.\n",
        "\n",
        "This would show a negative correlation.\n",
        "\n",
        "14.How to find correlation in Python?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Display correlation between two variables\n",
        "print(df['temperature'].corr(df['heating_bill']))\n",
        "\n",
        "# Or visualize it\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "15.What is causation?\n",
        "Causation means that one variable directly affects another — a cause-and-effect relationship.\n",
        "\n",
        "Example:\n",
        "More exercise → causes → improved health\n",
        "\n",
        "You do something, and something changes as a result.\n",
        "\n",
        " differnce Between Correlation and Causation\n",
        "Aspect\tCorrelation\tCausation\n",
        "Meaning\tVariables move together\tOne variable causes a change in another\n",
        "Implies cause?\t❌ No\t✅ Yes\n",
        "Direction\tCan be positive, negative, or zero\tImplies a directional effect\n",
        "Evidence\tStatistical relationship only\tRequires controlled experiment or theory\n",
        "\n",
        "❗ Example:\n",
        "Correlation: Ice cream sales and drowning incidents increase in summer.\n",
        "\n",
        "Are they correlated? ✅ Yes\n",
        "\n",
        "Does ice cream cause drowning? ❌ No\n",
        "\n",
        "Hidden factor (temperature) causes both.\n",
        "\n",
        "16.What is an Optimizer?\n",
        "\n",
        "-->An optimizer is an algorithm that adjusts the parameters (weights and biases) of a model to minimize the loss function during training.\n",
        "\n",
        "Why is an Optimizer Important?\n",
        "During training:\n",
        "\n",
        "The model makes predictions.\n",
        "\n",
        "A loss function measures how wrong the predictions are.\n",
        "\n",
        "The optimizer updates model parameters to reduce this loss.\n",
        "\n",
        "🔹 Types of Optimizers (with Examples)\n",
        "Here are the most common optimizers used in machine learning and deep learning:\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "Basic idea: Compute the gradient of the loss function and move in the direction that minimally reduces loss.\n",
        "\n",
        "17.What is sklearn.linear_model?\n",
        "sklearn.linear_model is a module in Scikit-learn that contains linear models for regression and classification tasks.\n",
        "\n",
        "18.What does model.fit() do?\n",
        "model.fit(X, y) is used to train the model on the input data (X) and the corresponding target (y).\n",
        "\n",
        "➕ It does:\n",
        "Finds the best parameters (weights) for the model.\n",
        "\n",
        "Minimizes the loss function (like MSE for regression).\n",
        "\n",
        "Prepares the model for making predictions using .predict().\n",
        "\n",
        "✅ Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)  # Train the model\n",
        "🔹 What arguments must be given to model.fit()?\n",
        "➤ Required arguments:\n",
        "Argument\tDescription\n",
        "X\tFeature matrix (2D array, shape: [n_samples, n_features])\n",
        "y\tTarget variable (1D array for regression, 1D or 2D for classification)\n",
        "\n",
        "19.What does model.predict() do?\n",
        "After a model is trained using model.fit(), you use model.predict() to:\n",
        "\n",
        "Make predictions on new (unseen) data using the model’s learned parameters.\n",
        "\n",
        "It returns the predicted outputs (target values) for the input features you provide.\n",
        "\n",
        "✅ Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Assuming the model has already been trained\n",
        "predictions = model.predict(X_test)\n",
        "X_test is the new input data\n",
        "\n",
        "predictions is the array of predicted target values\n",
        "\n",
        "🔹 What arguments must be given to model.predict()?\n",
        "➤ Required Argument:\n",
        "Argument\tDescription\n",
        "X\tInput data (features only) for which you want predictions\n",
        "\n",
        "Must be in the same format and number of features as used during training.\n",
        "\n",
        "Shape: [n_samples, n_features] (e.g., a 2D array or DataFrame)\n",
        "\n",
        "🔍 Example with Regression:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on new data\n",
        "y_pred = model.predict(X_test)\n",
        "🔍 Example with Classification:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict class labels\n",
        "y_pred = model.predict(X_test)\n",
        "🔹 Summary:\n",
        "Function\tPurpose\tRequired Input\n",
        "model.predict()\tGenerates predictions on new data\tX (features only)\n",
        "\n",
        "20.1. What are Continuous and Categorical Variables?\n",
        "-->✅ Continuous Variables:\n",
        "Numerical values that can take any value within a range.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Height (e.g., 170.5 cm)\n",
        "\n",
        "Temperature (e.g., 22.3°C)\n",
        "\n",
        "Salary, Age, Distance\n",
        "\n",
        "✅ Categorical Variables:\n",
        "Represent categories or labels.\n",
        "\n",
        "Can be nominal (no order) or ordinal (ordered).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Gender (Male, Female)\n",
        "\n",
        "Color (Red, Green, Blue)\n",
        "\n",
        "Education Level (High School < Bachelor < Master)\n",
        "\n",
        "21.What is Feature Scaling?\n",
        "Feature scaling is the process of normalizing or standardizing numerical features so that they are on a similar scale.\n",
        "\n",
        "❗ Why it matters:\n",
        "Many machine learning models (e.g., KNN, SVM, Logistic Regression, Gradient Descent-based models) are sensitive to feature scale.\n",
        "\n",
        "Features with large values can dominate those with smaller values if not scaled.\n",
        "\n",
        " How Does Feature Scaling Help in machine learning?\n",
        "Improves convergence of gradient descent.\n",
        "\n",
        "Reduces model bias toward large-valued features.\n",
        "\n",
        "Makes distance-based models (KNN, K-Means) work correctly.\n",
        "\n",
        "22.4. How to Perform Scaling in Python?\n",
        "\n",
        "-->from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # X is your feature matrix\n",
        "\n",
        "23.What is sklearn.preprocessing?\n",
        "-->It's a Scikit-learn module that provides tools to prepare your data for machine learning:\n",
        "\n",
        "Encoders: OneHotEncoder, LabelEncoder\n",
        "\n",
        "Scalers: StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "Imputers: SimpleImputer (for missing values)\n",
        "\n",
        "✅ Helps automate data preprocessing steps in pipelines\n",
        "\n",
        "24.How to Split Data for Model Fitting (Training & Testing)?\n",
        "\n",
        "-->from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "X: features\n",
        "\n",
        "y: target\n",
        "\n",
        "test_size: fraction of data for testing (e.g., 0.2 = 20%)\n",
        "\n",
        "random_state: for reproducibility\n",
        "\n",
        "25.What is Data Encoding?\n",
        "-->.Data encoding means converting categorical variables into numerical format so that machine learning models can understand them.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uIfw4AmKdMSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g-AtC-ekdSfh"
      }
    }
  ]
}